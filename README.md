# Motion Computer Control

Суть проекта заключается в разработке приложения, 
позволяющего контролировать некоторые действия компьютера с помощью жестов. 
Поступающее с веб-камеры изображение должно обрабатываться заранее обученной моделью, 
а затем, в зависимости от конкретного распознанного жеста, 
должна срабатывать та или иная последовательность заранее заданных действий.

## Требования:

1. Модель должна работать в реальном времени на среднем ноутбуке — соответственно, нужна хорошая скорость инференса.  
2. Модель должна распознавать жесты. Соответственно, классов в данной задаче может быть много, и они должны хорошо распознаваться.  
3. Жесты должны распознаваться стабильно, и единичные ложные срабатывания не должны влиять на работу программы.
4. Помимо этого должна быть возможность распознавать комбинацию жестов или движений рукой — чтобы сложные движения тоже могли быть восприняты как команды.  
5. Также нужен отдельный функционал, с помощью которого конкретный жест или комбинация жестов переводится в команду или набор команд.  

## План работы:

- ~~1-ый день. Подготовить проект к началу работы (git, docker).~~  
- 2-ой день. Выбрать предобученные модели. Обернуть их в свой класс для единообразия интерфейса.  
- 3-ий день. Сделать функционал для распознавания жестов. Сделать тестовый вариант программы.  
- 4-ый, 5-ый, 6-ой дни. Сделать интерфейс на PyQT. Интерфейс должен иметь 3 панели: 
    1. Для дообучения модели.
    2. Для её тестирования и экспорта в формат onnx.
    3. Для объединения конкретных жестов с конкретными командами (причём жесты могут запускать как упрощённые команды (конкретные сочетания клавиш), так и запускать отдельные python-файлы) и запуска приложения.  
- 7-ой и 8-ой дни. Реализовать функционал интерфейса.
- 9-ый и 10-ый дни. Тестирование, исправление багов, рефакторинг (если нужно).  

## Ход работы.

### Запуск приложения:

Для тестирования приложения используется docker. В директории проекта открываем консоль и сначала используем

`docker build -t <название_образа> .`

Затем

`docker run <название_образа>`

### Выбор и обучение модели:

Т.к. скорость инференса в решаемой задаче должна быть высокой, то выбор модели встал между 
YOLO и SSDLite на MobileNetv3. В конце концов было решено реализовать функционал и с той, и с той моделью.  
Изначально предполагалось, что будет взята стандартная модель, обученная на датасете COCO, 
и далее она будет дообучена на выборке с изображениями, содержащими размеченные жесты. 
Но как выяснилось, в этом нет необходимости: такая задача уже решалась, 
и здесь (https://github.com/hukenovs/hagrid?tab=readme-ov-file#train) 
можно найти как выборку с жестами, так и обученные baseline-модели. 
Поэтому теперь задача состоит не в том, чтобы взять модель, обученную на COCO, 
и дообучить её на выборке с жестами, а в том, чтобы взять модель, обученную на выборке 
с жестами, и дообучить её конкретно под себя. Посмотреть, какие жесты с моей 
веб-камеры модели распознают плохо (с моим качеством изображения, с моим освещением и т.д.), 
и подправить качество именно на них.  
Модели YOLO и SSDLite неудобно использовать из pytorch, а из ultralytics можно использовать только YOLO. 
Поэтому для этих двух моделей (а также для моделей формата onnx) было решено реализовать обёртку в виде единого класса. 
Он представлен в файле GestureDetector.py и называется также.  

